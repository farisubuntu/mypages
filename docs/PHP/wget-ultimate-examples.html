<!DOCTYPE html>
<html lang="en">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link
	href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css"
	rel="stylesheet"
	integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65"
	crossorigin="anonymous">
<link rel="stylesheet" href="./prism.css">
<link rel="stylesheet" href="./prism.css">
<link rel="stylesheet" href="./styles.css">
<style>
	h1 {
		color: green;
	}

	pre {
		border: 1px dashed red;
	}
</style>
<body>

	<nav>
		<ul>
<li><a class="" href="../index.html">index</a></li>
<li class="heading"><span>CSS</span></li>
<ul>
<li><a class="" href="../CSS/css-media-and-rwd.html">css media and rwd</a></li>
<li><a class="" href="../CSS/css-naming-convention.html">css naming convention</a></li>
<li><a class="" href="../CSS/css_page_media.html">css page media</a></li>
<li><a class="" href="../CSS/flexbox-review.html">flexbox review</a></li>
<li><a class="" href="../CSS/grid-review.html">grid review</a></li>
<li><a class="" href="../CSS/notes_grid_or_flexbox.html">notes grid or flexbox</a></li>
<li><a class="" href="../CSS/rwd-snippets.html">rwd snippets</a></li>
<li><a class="" href="../CSS/sizing_items_in_css.html">sizing items in css</a></li>
</ul>
<li class="heading"><span>Github</span></li>
<ul>
<li><a class="" href="../Github/gcm.html">gcm</a></li>
<li><a class="" href="../Github/git-branching.html">git branching</a></li>
<li><a class="" href="../Github/git-flight-rules.html">git flight rules</a></li>
<li><a class="" href="../Github/git-gcm-configure.html">git gcm configure</a></li>
<li><a class="" href="../Github/git.html">git</a></li>
<li><a class="" href="../Github/gitpod.html">gitpod</a></li>
<li><a class="" href="../Github/integrate_github_with_wordpress.html">integrate github with wordpress</a></li>
<li><a class="" href="../Github/md-to-github.html">md to github</a></li>
</ul>
<li class="heading"><span>Leaf 3</span></li>
<ul>
<li><a class="" href="../Leaf-3/basic.html">basic</a></li>
<li><a class="" href="../Leaf-3/helpers.html">helpers</a></li>
<li><a class="" href="../Leaf-3/mvc.html">mvc</a></li>
</ul>
<li class="heading"><span>Network Net</span></li>
<ul>
<li><a class="" href="../Network_Net/anonsurf_vpn_install.html">anonsurf vpn install</a></li>
<li><a class="" href="../Network_Net/dd-wrt.html">dd wrt</a></li>
<li><a class="" href="../Network_Net/dd-wrt_multiple_bssids.html">dd wrt multiple bssids</a></li>
<li><a class="" href="../Network_Net/ddwrt_basic_wl_settings.html">ddwrt basic wl settings</a></li>
<li><a class="" href="../Network_Net/download_youtube_videos.html">download youtube videos</a></li>
<li><a class="" href="../Network_Net/riseup-vpn.html">riseup vpn</a></li>
</ul>
<li class="heading"><span>PHP</span></li>
<ul>
<li><a class="" href="22092023.html">22092023</a></li>
<li><a class="" href="LearningResources.html">LearningResources</a></li>
<li><a class="" href="edx_js_backend_frameworks.html">edx js backend frameworks</a></li>
<li><a class="" href="embed_lib.html">embed lib</a></li>
<li><a class="" href="express_mdn.html">express mdn</a></li>
<li><a class="" href="helpful_resources.html">helpful resources</a></li>
<li><a class="" href="html_forms.html">html forms</a></li>
<li><a class="" href="http_response_codes.html">http response codes</a></li>
<li><a class="" href="json_guide.html">json guide</a></li>
<li><a class="" href="local-servers.html">local servers</a></li>
<li><a class="" href="logical_gradation_of_developers.html">logical gradation of developers</a></li>
<li><a class="" href="markdown.html">markdown</a></li>
<li><a class="" href="namespace_review.html">namespace review</a></li>
<li><a class="" href="notes_for_proff_nodejs_express.html">notes for proff nodejs express</a></li>
<li><a class="" href="npm_quick_ref.html">npm quick ref</a></li>
<li><a class="" href="pandoc_quick_ref.html">pandoc quick ref</a></li>
<li><a class="" href="php_basics.html">php basics</a></li>
<li><a class="" href="php_iterators.html">php iterators</a></li>
<li><a class="active" href="wget-ultimate-examples.html">wget ultimate examples</a></li>
</ul>
<li class="heading"><span>Packaging</span></li>
<ul>
<li><a class="" href="../Packaging/apt.html">apt</a></li>
<li><a class="" href="../Packaging/composer.html">composer</a></li>
</ul>
<li class="heading"><span>ReviewQuestions</span></li>
<ul>
<li><a class="" href="../ReviewQuestions/strings_quotes.html">strings quotes</a></li>
</ul>
<li class="heading"><span>Symfony</span></li>
<ul>
<li><a class="" href="../Symfony/Encore.html">Encore</a></li>
<li class="heading"><span>Symfonycasts</span></li>
<ul>
<li><a class="" href="../Symfony/Symfonycasts/Composer.html">Composer</a></li>
<li><a class="" href="../Symfony/Symfonycasts/Dependency Injection and the art of services and containers.html">Dependency Injection and the art of services and containers</a></li>
<li><a class="" href="../Symfony/Symfonycasts/Symfony 6 Fundamentals: Services, Config & Environments.html">Symfony 6 Fundamentals: Services, Config & Environments</a></li>
</ul>
<li><a class="" href="../Symfony/chapter5_symfonyCats.html">chapter5 symfonyCats</a></li>
<li><a class="" href="../Symfony/symfony_and_http_fundamentals.html">symfony and http fundamentals</a></li>
<li><a class="" href="../Symfony/symfony_basics.html">symfony basics</a></li>
<li><a class="" href="../Symfony/twig_basics.html">twig basics</a></li>
<li><a class="" href="../Symfony/twig_cheatsheet.html">twig cheatsheet</a></li>
</ul>
<li class="heading"><span>javascript</span></li>
<ul>
<li><a class="" href="../javascript/js_cheatsheet.html">js cheatsheet</a></li>
</ul>
</ul>
	</nav>
	<article>
	<p><a href="https://www.thegeekstuff.com/2009/09/the-ultimate-wget-download-guide-with-15-awesome-examples/">https://www.thegeekstuff.com/2009/09/the-ultimate-wget-download-guide-with-15-awesome-examples/</a></p>
<blockquote>
<p>wget utility is the best option to download files from internet. wget can pretty much handle all complex download situations including large file downloads</p>
</blockquote>
<h1 id="the-ultimate-wget-download-guide-with-15-awesome-examples">The Ultimate Wget Download Guide With 15 Awesome Examples <a class="heading-anchor-permalink" href="#the-ultimate-wget-download-guide-with-15-awesome-examples">#</a></h1>
<p><img src="https://static.thegeekstuff.com/wp-content/uploads/2009/10/15-wget-examples-300x257.png" alt="15 Practical Examples to Download Images and Videos from Internet"></p>
<p><strong>wget</strong> handle all complex download situations including large file downloads, recursive downloads, non-interactive downloads, multiple file downloads etc.,</p>
<h3 id="1.-download-single-file-with-wget">1. Download Single File with wget <a class="heading-anchor-permalink" href="#1.-download-single-file-with-wget">#</a></h3>
<div class="box">
The following example downloads a single file from internet and stores in the current directory.
<p>$ wget <a href="http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2">http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2</a></p>
<p>While downloading it will show a progress bar with the following information:</p>
<ul>
<li>%age of download completion (for e.g. 31% as shown below)</li>
<li>Total amount of bytes downloaded so far (for e.g. 1,213,592 bytes as shown below)</li>
<li>Current download speed (for e.g. 68.2K/s as shown below)</li>
<li>Remaining time to download (for e.g. eta 34 seconds as shown below)</li>
</ul>
<p>Download in progress:</p>
<p>$ wget <a href="http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2">http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2</a>
Saving to: `strx25-0.9.2.1.tar.bz2.1’</p>
<p>31% [=================&gt; 1,213,592   68.2K/s  eta 34s</p>
<p>Download completed:</p>
<p>$ wget <a href="http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2">http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2</a>
Saving to: `strx25-0.9.2.1.tar.bz2’</p>
<p>100%[======================&gt;] 3,852,374   76.8K/s   in 55s</p>
<p>2009-09-25 11:15:30 (68.7 KB/s) - `strx25-0.9.2.1.tar.bz2’ saved [3852374/3852374]</p>
<h3 id="2.-download-and-store-with-a-different-file-name-using-wget--o">2. Download and Store With a Different File name Using wget -O <a class="heading-anchor-permalink" href="#2.-download-and-store-with-a-different-file-name-using-wget--o">#</a></h3>
<p>By default wget will pick the filename from the last word after last forward slash, which may not be appropriate always.</p>
<p><strong>Wrong:</strong> Following example will download and store the file with name: download_script.php?src_id=7701</p>
<p>$ wget <a href="http://www.vim.org/scripts/download_script.php?src_id=7701">http://www.vim.org/scripts/download_script.php?src_id=7701</a></p>
<p>Even though the downloaded file is in zip format, it will get stored in the file as shown below.</p>
<p>$ ls
download_script.php?src_id=7701</p>
<p><strong>Correct:</strong> To correct this issue, we can specify the output file name using the -O option as:</p>
<p>$ wget -O taglist.zip <a href="http://www.vim.org/scripts/download_script.php?src_id=7701">http://www.vim.org/scripts/download_script.php?src_id=7701</a></p>
<h3 id="3.-specify-download-speed-%2F-download-rate-using-wget-%E2%80%93limit-rate">3. Specify Download Speed / Download Rate Using wget –limit-rate <a class="heading-anchor-permalink" href="#3.-specify-download-speed-%2F-download-rate-using-wget-%E2%80%93limit-rate">#</a></h3>
<p>While executing the wget, by default it will try to occupy full possible bandwidth. This might not be acceptable when you are downloading huge files on production servers. So, to avoid that we can limit the download speed using the –limit-rate as shown below.</p>
<p>In the following example, the download speed is limited to 200k</p>
<p>$ wget --limit-rate=200k <a href="http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2">http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2</a></p>
<h3 id="4.-continue-the-incomplete-download-using-wget--c">4. Continue the Incomplete Download Using wget -c <a class="heading-anchor-permalink" href="#4.-continue-the-incomplete-download-using-wget--c">#</a></h3>
<p>Restart a download which got stopped in the middle using wget -c option as shown below.</p>
<p>$ wget -c <a href="http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2">http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2</a></p>
<p>This is very helpful when you have initiated a very big file download which got interrupted in the middle. Instead of starting the whole download again, you can start the download from where it got interrupted using option -c</p>
<p><strong>Note:</strong> If a download is stopped in middle, when you restart the download again without the option -c, wget will append .1 to the filename automatically as a file with the previous name already exist. If a file with .1 already exist, it will download the file with .2 at the end.</p>
<h3 id="5.-download-in-the-background-using-wget--b">5. Download in the Background Using wget -b <a class="heading-anchor-permalink" href="#5.-download-in-the-background-using-wget--b">#</a></h3>
<p>For a huge download, put the download in background using wget option -b as shown below.</p>
<p>$ wget -b <a href="http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2">http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2</a>
Continuing in background, pid 1984.
Output will be written to `wget-log’.</p>
<p>It will initiate the download and gives back the shell prompt to you. You can always check the status of the download using tail -f as shown below.</p>
<p>$ tail -f wget-log
Saving to: `strx25-0.9.2.1.tar.bz2.4’</p>
<pre><code> 0K .......... .......... .......... .......... ..........  1% 65.5K 57s
50K .......... .......... .......... .......... ..........  2% 85.9K 49s
</code></pre>
<p>100K … … … … …  3% 83.3K 47s
150K … … … … …  5% 86.6K 45s
200K … … … … …  6% 33.9K 56s
250K … … … … …  7%  182M 46s
300K … … … … …  9% 57.9K 47s</p>
<p>Also, make sure to review our previous <a href="https://www.thegeekstuff.com/2009/09/multitail-to-view-tail-f-output-of-multiple-log-files-in-one-terminal/">multitail article</a> on how to use tail command effectively to view multiple files.</p>
<h3 id="6.-mask-user-agent-and-display-wget-like-browser-using-wget-%E2%80%93user-agent">6. Mask User Agent and Display wget like Browser Using wget –user-agent <a class="heading-anchor-permalink" href="#6.-mask-user-agent-and-display-wget-like-browser-using-wget-%E2%80%93user-agent">#</a></h3>
<p>Some websites can disallow you to download its page by identifying that the user agent is not a browser. So you can mask the user agent by using –user-agent options and show wget like a browser as shown below.</p>
<p>$ wget --user-agent=“Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3” URL-TO-DOWNLOAD</p>
<h3 id="7.-test-download-url-using-wget-%E2%80%93spider">7. Test Download URL Using wget –spider <a class="heading-anchor-permalink" href="#7.-test-download-url-using-wget-%E2%80%93spider">#</a></h3>
<p>When you are going to do scheduled download, you should check whether download will happen fine or not at scheduled time. To do so, copy the line exactly from the schedule, and then add –spider option to check.</p>
<p>$ wget --spider DOWNLOAD-URL</p>
<p>If the URL given is correct, it will say</p>
<p>$ wget --spider download-url
Spider mode enabled. Check if remote file exists.
HTTP request sent, awaiting response… 200 OK
Length: unspecified [text/html]
<strong>Remote file exists</strong> and could contain further links,
but recursion is disabled – not retrieving.</p>
<p>This ensures that the downloading will get success at the scheduled time. But when you had give a wrong URL, you will get the following error.</p>
<p>$ wget --spider download-url
Spider mode enabled. Check if remote file exists.
HTTP request sent, awaiting response… 404 Not Found
<strong>Remote file does not exist – broken link!!!</strong></p>
<p>You can use the spider option under following scenarios:</p>
<ul>
<li>Check before scheduling a download.</li>
<li>Monitoring whether a website is available or not at certain intervals.</li>
<li>Check a list of pages from your bookmark, and find out which pages are still exists.</li>
</ul>
<h3 id="8.-increase-total-number-of-retry-attempts-using-wget-%E2%80%93tries">8. Increase Total Number of Retry Attempts Using wget –tries <a class="heading-anchor-permalink" href="#8.-increase-total-number-of-retry-attempts-using-wget-%E2%80%93tries">#</a></h3>
<p>If the internet connection has problem, and if the download file is large there is a chance of failures in the download. By default wget retries 20 times to make the download successful.</p>
<p>If needed, you can increase retry attempts using –tries option as shown below.</p>
<p>$ wget --tries=75 DOWNLOAD-URL</p>
<h3 id="9.-download-multiple-files-%2F-urls-using-wget--i">9. Download Multiple Files / URLs Using Wget -i <a class="heading-anchor-permalink" href="#9.-download-multiple-files-%2F-urls-using-wget--i">#</a></h3>
<p>First, store all the download files or URLs in a text file as:</p>
<p>$ cat &gt; download-file-list.txt
URL1
URL2
URL3
URL4</p>
<p>Next, give the download-file-list.txt as argument to wget using -i option as shown below.</p>
<p>$ wget -i download-file-list.txt</p>
<h3 id="10.-download-a-full-website-using-wget-%E2%80%93mirror">10. Download a Full Website Using wget –mirror <a class="heading-anchor-permalink" href="#10.-download-a-full-website-using-wget-%E2%80%93mirror">#</a></h3>
<p>Following is the command line which you want to execute when you want to download a full website and made available for local viewing.</p>
<p>$ wget --mirror -p --convert-links -P ./LOCAL-DIR WEBSITE-URL</p>
<ul>
<li>–mirror : turn on options suitable for mirroring.</li>
<li>-p : download all files that are necessary to properly display a given HTML page.</li>
<li>–convert-links : after the download, convert the links in document for local viewing.</li>
<li>-P ./LOCAL-DIR : save all the files and directories to the specified directory.</li>
</ul>
<h3 id="11.-reject-certain-file-types-while-downloading-using-wget-%E2%80%93reject">11. Reject Certain File Types while Downloading Using wget –reject <a class="heading-anchor-permalink" href="#11.-reject-certain-file-types-while-downloading-using-wget-%E2%80%93reject">#</a></h3>
<p>You have found a website which is useful, but don’t want to download the images you can specify the following.</p>
<p>$ wget --reject=gif WEBSITE-TO-BE-DOWNLOADED</p>
<h3 id="12.-log-messages-to-a-log-file-instead-of-stderr-using-wget--o">12. Log messages to a log file instead of stderr Using wget -o <a class="heading-anchor-permalink" href="#12.-log-messages-to-a-log-file-instead-of-stderr-using-wget--o">#</a></h3>
<p>When you wanted the log to be redirected to a log file instead of the terminal.</p>
<p>$ wget -o download.log DOWNLOAD-URL</p>
<h3 id="13.-quit-downloading-when-it-exceeds-certain-size-using-wget--q">13. Quit Downloading When it Exceeds Certain Size Using wget -Q <a class="heading-anchor-permalink" href="#13.-quit-downloading-when-it-exceeds-certain-size-using-wget--q">#</a></h3>
<p>When you want to stop download when it crosses 5 MB you can use the following wget command line.</p>
<p>$ wget -Q5m -i FILE-WHICH-HAS-URLS</p>
<p><strong>Note:</strong> This quota will not get effect when you do a download a single URL. That is irrespective of the quota size everything will get downloaded when you specify a single file. This quota is applicable only for recursive downloads.</p>
<h3 id="14.-download-only-certain-file-types-using-wget--r--a">14. Download Only Certain File Types Using wget -r -A <a class="heading-anchor-permalink" href="#14.-download-only-certain-file-types-using-wget--r--a">#</a></h3>
<p>You can use this under following situations:</p>
<ul>
<li>Download all images from a website</li>
<li>Download all videos from a website</li>
<li>Download all PDF files from a website</li>
</ul>
<p>$ wget -r -A.pdf <a href="http://url-to-webpage-with-pdfs/">http://url-to-webpage-with-pdfs/</a></p>
<h3 id="15.-ftp-download-with-wget">15. FTP Download With wget <a class="heading-anchor-permalink" href="#15.-ftp-download-with-wget">#</a></h3>
<p>You can use wget to perform FTP download as shown below.</p>
<p>Anonymous FTP download using Wget</p>
<p>$ wget ftp-url</p>
<p>FTP download using wget with username and password authentication.</p>
<p>$ wget --ftp-user=USERNAME --ftp-password=PASSWORD DOWNLOAD-URL</p>

	</article>
	<script src="./prism.css"></script>
	<script src="./prism.js"></script>
	<script>prism.Highlight();</script>
</body>

</html>